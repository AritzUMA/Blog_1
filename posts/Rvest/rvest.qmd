---
title: 'Rvest package fof Web Scraping: Newspaper example'
date: '2022-06-01'
categories: ['123', 'Second Tag']
description: 'Webscraping'
execute: 
  message: false
  warning: false
editor_options: 
  chunk_output_type: console
---

![](rvest.png)

# Ethic Webscraping

The first thing we have to know is that the web sraping is a automatization of the webpages publiclky avaible information. There is a several legislations about this tecnique, like the [GDPR](https://gdpr.eu/) in Europe, in wich is delimeted for the Personal data or Personally Identifiablke Information (PII). So, we have to avoid the problematic question of the scraping personal data of european citizens and if we have to do it, follow the legals questions regarded in the GDPR.

Apart from the legal cuestion, it is impiortant to know that webscraping is technique that relantizes the servers of the companies if we do it very agressivly. Is for this that we have to priorice the APIs (Aplication Programminf Interface) avaiable for the webpages if there are. This aplications are prepared to dowload the information and don't disturb the normal function of the servers of the webpages.\
\
Knowing this information, we can start with the first of the 4 packages to webscraping information in R: Rvest.

## First steps

The rvest package have his own [website](https://rvest.tidyverse.org/) and is avaiable the full documentation in the [CRAN](https://cran.r-project.org/web/packages/rvest/rvest.pdf) repository. Is a very easy package to use and to deal with, in wich the principal focus is in the html structure and there csss, tags or Xpaths between others. Lets try to do this easy tutorial with a [naiz.eus](https://www.naiz.eus/) webpage and the news that are avaiable.

## 1 - Install and load the library

Like always, we need install the pacakge.

```{r}
install.packages("rvest")
```

Once the package is installed, we need "call" the package to use their functions.

```{r}
library(rvest)
library(tidyverse)
```

## 2 - Read the page

First of all we need to read the page in pur local machine, is for this that we need to use read_html() function to save the html structure in the variable.

```{r}
News_paper <- read_html("https://www.naiz.eus/")

print(News_paper)
```

## 3 - Find the CSS

Like we see, we have the newspaper html structure in our variable. Once this is donne, we need to extract the css responsabile for the titles of the newspaper. For this task we can do it manually, clicking in the right button on the mouse and selecting the "inspect" option we can find manually the common css for all the titles of the articles.

![](CSS_manually.png) We can see that the common class or css por the articles is article, but if we move the mouse on the articles, we will see that the css us for all the article, no only the title. This way we need to split the article class to see how it the article divided in different classes to see what css is the correct one. We will see something like this image, in wich each article is separated in different css

![](CSS_manually_title.png) Like we see, the css "article\_\_title" is our appropiate css we need to use to extract the title of the article.

Other way to do this more easy, is to install add ons we can use for this task (I use [SelectorHub](#0) for Firefox and [Selectorgadget](#0) for chrome navigator). This way we can select easier the css we want to dowload. In our case is

## 4 - Extract the css information

First of all we have to ensure that our css is apropuiate. For this lets try first to extract the first title. For this, first we have to stablish the csss like a node in wich we want extract and then extract the text of the node we determinate.

```{r}
News_paper_title <- News_paper %>% html_node(".article__title") %>% html_text()
print(News_paper_title)
```

We can see the the first title of the webpage, lets see if how we can get the second one.

```{r}
News_paper_title_second <- News_paper %>% html_node(".article__title:nth-child(2)") %>% html_text()
print(News_paper_title)
```

We can access to the "childs" of the css this way.

Now, we want all the titles toguether in one variable. Lets try to find trought the SelectorGadget in google chrome. For this goal we need to find apropiate css selecting with the add on. Once we see all the titles selected, we can try if all is correct with the selected css.

![](CSS_selector.png)

```{r}
News_paper_titles <- News_paper %>% html_nodes(".article__title a") %>% html_text()
News_paper_titles
```

Now we can use one of multiples packages avaible to clean the text, but to do it simple, we can do it ease witn the function gsub()

```{r}
News_paper_titles <- gsub("\n        \n|\n        ","",News_paper_titles)
News_paper_titles
```

Now we have vector with all the titles of the newspaper of the specific day.

## 5 - Activities

If you are still

-   Try to extract all the links for each title (Tip, use htlm_attr instead and html_text("href"))

-   Try to extract the author of each new, and figure out what is the problem with the lenght of the vector

-   Try to extract the lenguage of each new (What news are in basque and wich ones are in spanish? Do it thinking to do it automaticlly)

-   Try to save all the data in a data frame, each information of the new for each title

Thanks for see the article, and share it if you like it.
